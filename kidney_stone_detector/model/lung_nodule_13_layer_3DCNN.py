'''
12 3D CNN layers
described here : https://biomedicalcomputervision.uniandes.edu.co/images/papers/pa_sipaim2017.pdf
open source code from: https://github.com/BCV-Uniandes/LungCancerDiagnosis-pytorch

'''

import torch
import torch.nn as nn
import torch.nn.functional as F

class Net(nn.Module):
    def __init__(self, dropout_rate=0.65):
        super(Net, self).__init__()
        self.conv1 = nn.Conv3d(1, 64, kernel_size=3, padding=1)
        self.bn1 = nn.BatchNorm3d(64)
        self.relu1 = nn.ReLU()
        self.do1 = nn.Dropout3d(p=dropout_rate)
        self.conv2 = nn.Conv3d(64, 64, kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm3d(64)
        self.relu2 = nn.ReLU()
        self.do2 = nn.Dropout3d(p=dropout_rate)
        self.conv3 = nn.Conv3d(64, 64, kernel_size=3, padding=1)
        self.bn3 = nn.BatchNorm3d(64)
        self.relu3 = nn.ReLU()
        self.do3 = nn.Dropout3d(p=dropout_rate)
        self.pool1 = nn.MaxPool3d(kernel_size=2, stride=2, padding=0)
        self.conv4 = nn.Conv3d(64, 64, kernel_size=3, padding=1)
        self.bn4 = nn.BatchNorm3d(64)
        self.relu4 = nn.ReLU()
        self.do4 = nn.Dropout3d(p=dropout_rate)
        self.conv5 = nn.Conv3d(64, 64, kernel_size=3, padding=1)
        self.bn5 = nn.BatchNorm3d(64)
        self.relu5 = nn.ReLU()
        self.do5 = nn.Dropout3d(p=dropout_rate)
        self.conv6 = nn.Conv3d(64, 64, kernel_size=3, padding=1)
        self.bn6 = nn.BatchNorm3d(64)
        self.relu6 = nn.ReLU()
        self.do6 = nn.Dropout3d(p=dropout_rate)
        self.conv7 = nn.Conv3d(64, 64, kernel_size=3, padding=1)
        self.bn7 = nn.BatchNorm3d(64)
        self.relu7 = nn.ReLU()
        self.do7 = nn.Dropout3d(p=dropout_rate)
        self.pool2 = nn.MaxPool3d(kernel_size=2, stride=2, padding=0)
        self.conv8 = nn.Conv3d(64, 64, kernel_size=3, padding=1)
        self.bn8 = nn.BatchNorm3d(64)
        self.relu8 = nn.ReLU()
        self.do8 = nn.Dropout3d(p=dropout_rate)
        self.conv9 = nn.Conv3d(64, 64, kernel_size=3, padding=1)
        self.bn9 = nn.BatchNorm3d(64)
        self.relu9 = nn.ReLU()
        self.do9 = nn.Dropout3d(p=dropout_rate)
        self.conv10 = nn.Conv3d(64, 64, kernel_size=3, padding=1)
        self.bn10 = nn.BatchNorm3d(64)
        self.relu10 = nn.ReLU()
        self.do10 = nn.Dropout3d(p=dropout_rate)
        self.conv11 = nn.Conv3d(64, 64, kernel_size=3, padding=1)
        self.bn11 = nn.BatchNorm3d(64)
        self.relu11 = nn.ReLU()
        self.do11 = nn.Dropout3d(p=dropout_rate)
        self.conv12 = nn.Conv3d(64, 64, kernel_size=3, padding=1)
        self.bn12 = nn.BatchNorm3d(64)
        self.relu12 = nn.ReLU()
        self.do12 = nn.Dropout3d(p=dropout_rate)
        self.conv13 = nn.Conv3d(64, 64, kernel_size=6, padding=0)
        self.bn13 = nn.BatchNorm3d(64)
        self.relu13 = nn.ReLU()
        self.do13 = nn.Dropout3d(p=dropout_rate)
        #self.fc1 = nn.Linear(64*6*6*6, 64)
        #self.relu13 = nn.ReLU()
        self.fc2 = nn.Linear(64, 1)
        #self.softmax = nn.Softmax()

    def forward(self, x):
        out = self.do1(self.relu1(self.bn1(self.conv1(x))))
        out = self.do2(self.relu2(self.bn2(self.conv2(out))))
        out = self.do3(self.pool1(self.relu3(self.bn3(self.conv3(out)))))
        print(out.shape)
        out = self.do4(self.relu4(self.bn4(self.conv4(out))))
        print(out.shape)
        out = self.do5(self.relu5(self.bn5(self.conv5(out))))
        print(out.shape)
        out = self.do6(self.relu6(self.bn6(self.conv6(out))))
        print(out.shape)
        out = self.do7(self.pool2(self.relu7(self.bn7(self.conv7(out)))))
        out = self.do8(self.relu8(self.bn8(self.conv8(out))))
        out = self.do9(self.relu9(self.bn9(self.conv9(out))))
        out = self.do10(self.relu10(self.bn10(self.conv10(out))))
        out = self.do11(self.relu11(self.bn11(self.conv11(out))))
        out = self.do12(self.relu12(self.bn12(self.conv12(out))))
        out = self.do13(self.relu13(self.bn13(self.conv13(out))))
        out = self.fc2(out.view(-1, 64))
        #out = self.fc2(self.fc1((out.view(-1, 64)))
        #out = self.softmax(out)
        return out
